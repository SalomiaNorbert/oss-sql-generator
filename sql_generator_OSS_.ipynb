{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Runtime → Change runtime type → make sure GPU is selected.**"
      ],
      "metadata": {
        "id": "JzbRoh9VbYvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Install libraries**"
      ],
      "metadata": {
        "id": "7oYo3o8vbdHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers accelerate bitsandbytes sqlparse pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNtHjALSbovE",
        "outputId": "3653d0e1-9c71-4585-b04f-cf17ea794240"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Verify GPU + memory (helps us pick 8-bit vs fp16)**"
      ],
      "metadata": {
        "id": "Zq-pAPhJbzXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, math\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    m = torch.cuda.get_device_properties(0).total_memory\n",
        "    print(\"GPU total memory (GB):\", round(m/1e9, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCSLtVErboSr",
        "outputId": "7ef1334a-3c41-4451-9d39-06802ecc5b91"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU total memory (GB): 15.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Load tokenizer + model (memory-aware)**"
      ],
      "metadata": {
        "id": "Po0bOexYcUs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"defog/sqlcoder-7b-2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "use_8bit = True  # default safe; switch to fp16 if plenty of VRAM\n",
        "if torch.cuda.is_available():\n",
        "    total_mem = torch.cuda.get_device_properties(0).total_memory\n",
        "    # if you have ~20GB+ VRAM, fp16 is usually fine\n",
        "    if total_mem >= 20e9:\n",
        "        use_8bit = False\n",
        "\n",
        "if use_8bit:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        load_in_8bit=True,\n",
        "        use_cache=True,\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        trust_remote_code=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "# make sure we have a pad token (some LLMs don’t define one)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "540dc96fa69c4a49872566b4101b17d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "433877a3675940e2a941b7c5cfe8e133"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/515 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26ab0623f2844ebc9a60b2d0555f5fd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/691 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "759a8065c43741768d6e71a05d21f14d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d845ae5b3394135b321bcffa281a6d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc36670cec4243799266d52c560b8ef3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30b26cb2dd9d4c7eb9b1395dc689ba69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaf6b5d55f754c93aa13ccdb2e116657"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "356625df956440108067d44a672c261b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afcf146b0589472e8deda28f5344fa51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19a62a47f4fe4de6aca54f6bf4f908c8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Define your database schema (cleaned)**"
      ],
      "metadata": {
        "id": "0h4eLTiucjzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SCHEMA = \"\"\"\n",
        "CREATE TABLE products(\n",
        "  product_id INTEGER PRIMARY KEY,\n",
        "  name VARCHAR(50),\n",
        "  price DECIMAL(10,2),\n",
        "  quantity INTEGER\n",
        ");\n",
        "\n",
        "CREATE TABLE salespeople(\n",
        "  salesperson_id INTEGER PRIMARY KEY,\n",
        "  name VARCHAR(50),\n",
        "  region VARCHAR(50)\n",
        ");\n",
        "\n",
        "CREATE TABLE sales(\n",
        "  sale_id INTEGER PRIMARY KEY,\n",
        "  product_id INTEGER,\n",
        "  customer_id INTEGER,\n",
        "  salesperson_id INTEGER,\n",
        "  sale_date DATE,\n",
        "  quantity INTEGER\n",
        ");\n",
        "\n",
        "CREATE TABLE product_supplier(\n",
        "  supplier_id INTEGER PRIMARY KEY,\n",
        "  product_id INTEGER,\n",
        "  supply_price DECIMAL(10,2)\n",
        ");\n",
        "\n",
        "-- Joins:\n",
        "-- sales.product_id -> products.product_id\n",
        "-- sales.salesperson_id -> salespeople.salesperson_id\n",
        "-- product_supplier.product_id -> products.product_id\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "KQGeNI5acmq0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5) Build a solid prompt template**"
      ],
      "metadata": {
        "id": "ReeNqnbjcprM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT = \"\"\"### Task\n",
        "Generate a SQL query to answer the user's question using ONLY the provided database schema.\n",
        "\n",
        "### Rules\n",
        "- If the question cannot be answered with the schema, return exactly: I do not know\n",
        "- revenue = price * quantity\n",
        "- cost = supply_price * quantity\n",
        "- Return only the SQL after the [SQL] tag.\n",
        "\n",
        "### Database Schema\n",
        "{schema}\n",
        "\n",
        "### Question\n",
        "{question}\n",
        "\n",
        "### Answer\n",
        "[SQL]\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Q9fkvzykcsL6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6) Helper to extract/format SQL**"
      ],
      "metadata": {
        "id": "6Q48wI5rczHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, sqlparse\n",
        "\n",
        "def extract_sql(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Pulls SQL after the [SQL] tag. If not found, tries to find first SELECT.\n",
        "    Returns 'I do not know' if nothing usable is found.\n",
        "    \"\"\"\n",
        "    after = text.split(\"[SQL]\", 1)[-1] if \"[SQL]\" in text else text\n",
        "    # strip special tokens\n",
        "    after = after.replace(\"</s>\", \" \").replace(\"<s>\", \" \").strip()\n",
        "\n",
        "    # try to find a SQL-y start\n",
        "    m = re.search(r\"(SELECT|WITH)\\b.*\", after, flags=re.IGNORECASE | re.DOTALL)\n",
        "    if not m:\n",
        "        # sometimes models add backticks or code fences\n",
        "        m = re.search(r\"```(?:sql)?(.*)```\", after, flags=re.IGNORECASE | re.DOTALL)\n",
        "        if m:\n",
        "            candidate = m.group(1).strip()\n",
        "        else:\n",
        "            return \"I do not know\"\n",
        "    else:\n",
        "        candidate = m.group(0).strip()\n",
        "\n",
        "    # basic hard stop at first semicolon block (optional)\n",
        "    # keep multi-statement if needed; here we keep as-is\n",
        "    formatted = sqlparse.format(candidate, reindent=True)\n",
        "    return formatted if formatted else \"I do not know\"\n"
      ],
      "metadata": {
        "id": "5ITQApJAc2W5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7) The generate_query() function**"
      ],
      "metadata": {
        "id": "at0WibmMc9Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_query(question: str, max_new_tokens: int = 300) -> str:\n",
        "    prompt = PROMPT.format(schema=SCHEMA, question=question)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=False,            # deterministic for consistency\n",
        "        num_beams=1,                # beam=1 is fine for structured outputs\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "    text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
        "    sql = extract_sql(text)\n",
        "\n",
        "    # optional: guardrail — if the model didn’t produce SELECT/WITH, force \"I do not know\"\n",
        "    if not re.search(r\"^(SELECT|WITH)\\b\", sql, flags=re.IGNORECASE):\n",
        "        return \"I do not know\"\n",
        "    return sql\n"
      ],
      "metadata": {
        "id": "HTar8nQsdAqY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8) Test with a single question**"
      ],
      "metadata": {
        "id": "vJ-H8gkEdDCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What was the highest quantity sold last month?\"\n",
        "sql = generate_query(question)\n",
        "print(sql)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MVwiAoAdGV_",
        "outputId": "749e020b-e11b-4531-a18f-baf4d2baa6bc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT MAX(s.quantity) AS max_quantity\n",
            "FROM sales s\n",
            "WHERE s.sale_date >= (CURRENT_DATE - INTERVAL '1 month');\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7J6JWtGbdLiX"
      }
    }
  ]
}